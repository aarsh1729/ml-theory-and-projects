{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b5f07e",
   "metadata": {},
   "source": [
    "# machine learning basics cheat sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e242f",
   "metadata": {},
   "source": [
    "## linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6855c",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "A. Exploratory data analysis\n",
    "sns.pairplot(USAhousing) using seaborn look patterns\n",
    "sns.heatmap(USAhousing.corr()) correlation of X with y\n",
    "\n",
    "B. trainig model \n",
    "\n",
    "1. train test data split\n",
    "X = USAhousing[col_1, col_2,...]\n",
    "y = USAhousing[target variable]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101) test size is 0.3 to 0.4\n",
    "\n",
    "C. creating and testing model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "coeff_df = pd.DataFrame(lm.coef_,X.columns,columns=['Coefficient'])\n",
    "coeff_df will print out correlation of various input parameters with target variables\n",
    "for example : - Holding all other features fixed, a 1 unit increase in Avg. Area Income is associated with an **increase of $21.52 **.\n",
    "\n",
    "Holding all other features fixed, a 1 unit increase in Avg. Area House Age is associated with an **increase of $164883.28 **.\n",
    "D. testing the predicted against test data\n",
    "predictions = lm.predict(X_test)\n",
    "plt.scatter(y_test,predictions) or any other graph like residual histogram\n",
    "sns.distplot((y_test-predictions),bins=50)\n",
    "\n",
    "Important points regarding error analysis\n",
    "\n",
    "MAE is the easiest to understand, because it's the average error.\n",
    "MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f73674",
   "metadata": {},
   "source": [
    "## logistic regression ( used for yes and no questions)\n",
    "\n",
    "Steps\n",
    "\n",
    "A. Exploratory data analysis \n",
    "sns.pairplot(USAhousing) using seaborn look patterns \n",
    "\n",
    "we explore what data is missing using heatmaps and isnull\n",
    "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "\n",
    "if there are many missing data delete the columns \n",
    "if few are missing like 20 % then use average value or average value of subtypes(column) this can be done by simply looking at the box plots, or by using some mathematical method.\n",
    "\n",
    "We'll need to convert categorical features(like sex, or some category) to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.\n",
    "\n",
    "we use pandas for this:-\n",
    "sex = pd.get_dummies(train['Sex'],drop_first=True)\n",
    "\n",
    "then drop all the parameters which have no numerical significance or define numerical data in X for model.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.drop('Survived',axis=1),train['Survived'], test_size=0.30,random_state=101)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "predictions = logmodel.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076789d",
   "metadata": {},
   "source": [
    "## knn (used for classifying one datapoint to various datasets)\n",
    "\n",
    "Standardize the Variables\n",
    "Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Any variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df.drop('TARGET CLASS',axis=1))\n",
    "scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))\n",
    "df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])# without target class\n",
    "\n",
    "split data\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)# we can change n_neighbours\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test,pred))# here we chick performance of model\n",
    "\n",
    "now we optimize the value of n_neighbour by comparing the prediction data against test data where they were not equal\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')\n",
    "\n",
    "hence k versus error rate was graphed and we may conclude that after n_neighbour > 23 almost same error rate is present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15b786",
   "metadata": {},
   "source": [
    "## decision tree and random forest( again used for yes and no decesions)\n",
    "\n",
    "In order to pick which feature to split on, we need a way of measuring how good the split is. This is where information gain and entropy come in.\n",
    "\n",
    "We would like to choose questions that give a lot of information about the tree’s prediction. \n",
    "For example, if there is a single yes/no question that accurately predicts the outputs 99% of the time, then that question allows us to “gain” a lot of information about our data. In order to measure how much information we gain, we introduce entropy.\n",
    "\n",
    "The entropy is a measure of uncertainty associated with our data.\n",
    "\n",
    "We can intuitively think that if a data set had only one label (e.g. every passenger survived), then we have a low entropy. So we would like to split our data in a way that minimizes the entropy. The better the splits, the better our prediction will be.\n",
    "\n",
    "For the pros, Decision Trees are easily interpretable and can handle missing values and outliers. They can also handle discrete and continuous data types, along with irrelevant features.\n",
    "\n",
    "For the cons, Decision Trees can be very easy to overfit, and while they are computationally cheap for prediction, training the decision tree can be computationally expensive.\n",
    "\n",
    "The idea behind a **Random Forest** is actually pretty simple: We repeatedly select data from the data set (with replacement) and build a Decision Tree with each new sample\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train,y_train)\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "**we can even print out the decisioon tree using pydt library**\n",
    "\n",
    "similar code for random forest \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=100)# 100 random trees used\n",
    "rfc.fit(X_train, y_train)\n",
    "print(classification_report(y_test,rfc_pred))# slightly better performance of random forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdc81c",
   "metadata": {},
   "source": [
    "## support vector machines\n",
    "\n",
    "these are mainly used for classification, regression\n",
    "by making a hyperplane to an n-dimensional dataset and measuring the error in that dataset.\n",
    "Its used when there are many data points and performs much better than decsion trees when many parameters are considered.\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "**confusion matrix\n",
    "TP,FP\n",
    "FN,TN**\n",
    "if predictions are 0 in either category we have to use grid search\n",
    "this may happen because the data higly biased\n",
    "\n",
    "\n",
    "Finding the right parameters (like what C or gamma values to use) is a tricky task! But luckily, we can be a little lazy and just try a bunch of combinations and see what works best! This idea of creating a 'grid' of parameters and just trying out all the possible combinations is called a Gridsearch, this method is common enough that Scikit-learn has this functionality built in with GridSearchCV! The CV stands for cross-validation which is the\n",
    "\n",
    "GridSearchCV takes a dictionary that describes the parameters that should be tried and a model to train. The grid of parameters is defined as a dictionary, where the keys are the parameters and the values are the settings to be tested.\n",
    "\n",
    "**It basically adjusts the hyperparameters of a model and gives the parameters with the most relevant values of these paameters.**\n",
    "\n",
    "param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \n",
    "\n",
    "here we vary the parameters c and gamma.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\n",
    "\n",
    "One of the great things about GridSearchCV is that it is a meta-estimator. It takes an estimator like SVC, and creates a new estimator, that behaves exactly the same - in this case, like a classifier. You should add refit=True and choose verbose to whatever number you want, higher the number, the more verbose (verbose just means the text output describing the process).\n",
    "\n",
    "What fit does is a bit more involved then usual. First, it runs the same loop with cross-validation, to find the best parameter combination. Once it has the best combination, it runs fit again on all data passed to fit (without cross-validation), to built a single new model using the best parameter setting.\n",
    "\n",
    "grid.best_params_ # this gives the best parameter pair values\n",
    "\n",
    "grid.best_estimator_ # this is now best svc for the data and similar things can be done on this model like on the svc \n",
    "\n",
    "grid_predictions = grid.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,grid_predictions)) # we get a much better result using this.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca287a4f",
   "metadata": {},
   "source": [
    "## k means clustering(unsupervised learning)- no target to predict\n",
    "\n",
    "K-means is an unsupervised learning method for clustering data points. The algorithm iteratively divides data points into K clusters by minimizing the variance in each cluster.\n",
    "\n",
    "First, each data point is randomly assigned to one of the K clusters. Then, we compute the centroid (functionally the center) of each cluster, and reassign each data point to the cluster with the closest centroid. We repeat this process until the cluster assignments for each data point are no longer changing.\n",
    "\n",
    "K-means clustering requires us to select K, the number of clusters we want to group the data into. The elbow method lets us graph the inertia (a distance-based metric) and visualize the point at which it starts decreasing linearly. This point is referred to as the \"eblow\" and is a good estimate for the best value for K based on our data.\n",
    "\n",
    "A bank wants to give credit card offers to its customers. Currently, they look at the details of each customer and, based on this information, decide which offer should be given to which customer.\n",
    "\n",
    "Now, the bank can potentially have millions of customers. Does it make sense to look at the details of each customer separately and then make a decision? Certainly not! It is a manual process and will take a huge amount of time.\n",
    "\n",
    "So what can the bank do? One option is to segment its customers into different groups. For instance, the bank can group the customers based on their income, basically create clusters.\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(data[0])\n",
    "\n",
    "comparison k means vs actual data\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))\n",
    "ax1.set_title('K Means')\n",
    "ax1.scatter(data[0][:,0],data[0][:,1],c=kmeans.labels_,cmap='rainbow')\n",
    "ax2.set_title(\"Original\")\n",
    "ax2.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')\n",
    "\n",
    "The k value in k-means clustering is a crucial parameter that determines the number of clusters to be formed in the dataset. Finding the optimal k value in the k-means clustering can be very challenging, especially for noisy data. The appropriate value of k depends on the data structure and the problem being solved. It is important to choose the right value of k, as a small value can result in under-clustered data, and a large value can cause over-clustering.\n",
    "\n",
    "for optimizing the value of k we use the elbow method\n",
    "\n",
    "data = list(zip(x, y))\n",
    "inertias = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(data)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,11), inertias, marker='o')\n",
    "plt.title('Elbow method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca14ac4",
   "metadata": {},
   "source": [
    "## natural language processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c868f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39margmax(arr)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "arr = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n",
    "np.argmax(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcac6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "range(10,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8cc702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee8286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
